[
  {
    "objectID": "course_schedule.html",
    "href": "course_schedule.html",
    "title": "",
    "section": "",
    "text": "Course Schedule\nDetailed schedule information with all asynchronous materials, assignment instructions and due dates is provided on Google Classroom.\n\n\n\n\n\n\n\n\n\n\n\n\n\nSession\n12/21/22\nTutorials to Review\nTutorial 1 - 5\nHomework Due\n\n\nAsync Materials (to be completed PRIOR tosessions) | Challenge (submit before session - reviewed in session) | ====================================================+=========================================================+ Pre-session materials, Data import | 1: Challenge 1 Read data + HW1 setup github |\n\n\n\n\n12/26/22\nTutorial 7&9\n\n\nGroup and Summarize\n2: Summary Statistics\n\n\n\n12/28/22\n\n\n\n\nTidy Data\n3: Read + pivot data\n\n\n\n1/2/23 |   |   | Data wrangling and transformation | 4: Mutate with lubridate and stringr |\n\n\n1/4/23 |   | HW2  |   | HW2 |\n\n\n1/9/23 | Tutorial  6&8 |   | Visualizing statistics | Univariate graph, Bivariate graph |\n\n\n1/11/23 |   |   | Visualizing time and groups | Graph over time, stacked bar chart |\n\n\n1/16/23 |   |   | Customizing ggplot | Axis Labels, facets |\n\n\n1/18/23 |   |   | Joins | Joins |\n\n\n1/23/23 |   | HW3 |   | HW3 |\n\n\n1/25/23 |   |   | Creating your own Functions | Create a function |\n\n\n1/30/23 |   |   | purrr and map | purrr |\n\n\n2/1/23 |   |  Final Project Due Feb. 5th | Intro to Python | optional | ———–+———————+————————–+—————————————————-+———————————————————+"
  },
  {
    "objectID": "about/about.html",
    "href": "about/about.html",
    "title": "Your Name",
    "section": "",
    "text": "##Instructions"
  },
  {
    "objectID": "about/about.html#educationwork-background",
    "href": "about/about.html#educationwork-background",
    "title": "Your Name",
    "section": "Education/Work Background",
    "text": "Education/Work Background"
  },
  {
    "objectID": "about/about.html#r-experience",
    "href": "about/about.html#r-experience",
    "title": "Your Name",
    "section": "R experience",
    "text": "R experience"
  },
  {
    "objectID": "about/about.html#research-interests",
    "href": "about/about.html#research-interests",
    "title": "Your Name",
    "section": "Research interests",
    "text": "Research interests"
  },
  {
    "objectID": "about/about.html#hometown",
    "href": "about/about.html#hometown",
    "title": "Your Name",
    "section": "Hometown",
    "text": "Hometown"
  },
  {
    "objectID": "about/about.html#hobbies",
    "href": "about/about.html#hobbies",
    "title": "Your Name",
    "section": "Hobbies",
    "text": "Hobbies"
  },
  {
    "objectID": "about/about.html#fun-fact",
    "href": "about/about.html#fun-fact",
    "title": "Your Name",
    "section": "Fun fact",
    "text": "Fun fact"
  },
  {
    "objectID": "about/TanmayAgrawal.html",
    "href": "about/TanmayAgrawal.html",
    "title": "Tanmay Agrawal",
    "section": "",
    "text": "##Instructions"
  },
  {
    "objectID": "about/TanmayAgrawal.html#educationwork-background",
    "href": "about/TanmayAgrawal.html#educationwork-background",
    "title": "Tanmay Agrawal",
    "section": "Education/Work Background",
    "text": "Education/Work Background\nBachelor’s: - SRM University: Computer Science - UC Berkeley: Computer Science, Data Science\nMaster’s: - UMass Amherst: Computer Science\nWork: Deep Learning Software Intern: NVIDIA ML Intern: PathAI, Inc."
  },
  {
    "objectID": "about/TanmayAgrawal.html#r-experience",
    "href": "about/TanmayAgrawal.html#r-experience",
    "title": "Tanmay Agrawal",
    "section": "R experience",
    "text": "R experience\nI have minimal experience with R. I first used it during my sophomore year of undergrad (~4 years ago) for a project and I really enjoyed it. The R community has grown so much and the IDE, the user experience seems much better now."
  },
  {
    "objectID": "about/TanmayAgrawal.html#research-interests",
    "href": "about/TanmayAgrawal.html#research-interests",
    "title": "Tanmay Agrawal",
    "section": "Research interests",
    "text": "Research interests\nBroadly, I am interested in applying Data Science and ML for social good causes."
  },
  {
    "objectID": "about/TanmayAgrawal.html#hometown",
    "href": "about/TanmayAgrawal.html#hometown",
    "title": "Tanmay Agrawal",
    "section": "Hometown",
    "text": "Hometown\nIndore, India"
  },
  {
    "objectID": "about/TanmayAgrawal.html#hobbies",
    "href": "about/TanmayAgrawal.html#hobbies",
    "title": "Tanmay Agrawal",
    "section": "Hobbies",
    "text": "Hobbies\nLearning new things, watching chess, coding"
  },
  {
    "objectID": "about/TanmayAgrawal.html#fun-fact",
    "href": "about/TanmayAgrawal.html#fun-fact",
    "title": "Tanmay Agrawal",
    "section": "Fun fact",
    "text": "Fun fact\nI have a fraternal twin and we have rhyming names."
  },
  {
    "objectID": "about/AboutCristhianBarbaGarzon.html",
    "href": "about/AboutCristhianBarbaGarzon.html",
    "title": "Cristhian Barba Garzon",
    "section": "",
    "text": "##Instructions"
  },
  {
    "objectID": "about/AboutCristhianBarbaGarzon.html#educationwork-background",
    "href": "about/AboutCristhianBarbaGarzon.html#educationwork-background",
    "title": "Cristhian Barba Garzon",
    "section": "Education/Work Background",
    "text": "Education/Work Background\nMajor: Physics Major Year: 3rd Year Work: Student Researcher"
  },
  {
    "objectID": "about/AboutCristhianBarbaGarzon.html#r-experience",
    "href": "about/AboutCristhianBarbaGarzon.html#r-experience",
    "title": "Cristhian Barba Garzon",
    "section": "R experience",
    "text": "R experience\nExperience Level: None"
  },
  {
    "objectID": "about/AboutCristhianBarbaGarzon.html#research-interests",
    "href": "about/AboutCristhianBarbaGarzon.html#research-interests",
    "title": "Cristhian Barba Garzon",
    "section": "Research interests",
    "text": "Research interests\nI am interested in working with code and learning how to manipulate and visualize data."
  },
  {
    "objectID": "about/AboutCristhianBarbaGarzon.html#hometown",
    "href": "about/AboutCristhianBarbaGarzon.html#hometown",
    "title": "Cristhian Barba Garzon",
    "section": "Hometown",
    "text": "Hometown\nLowell, MA"
  },
  {
    "objectID": "about/AboutCristhianBarbaGarzon.html#hobbies",
    "href": "about/AboutCristhianBarbaGarzon.html#hobbies",
    "title": "Cristhian Barba Garzon",
    "section": "Hobbies",
    "text": "Hobbies\n\njump roping\ncooking\nsleeping\nlearning to code\nhiking (trying to get into it)"
  },
  {
    "objectID": "about/AboutCristhianBarbaGarzon.html#fun-fact",
    "href": "about/AboutCristhianBarbaGarzon.html#fun-fact",
    "title": "Cristhian Barba Garzon",
    "section": "Fun fact",
    "text": "Fun fact\n\nI am the first in my family to go to college and nearly graduate!"
  },
  {
    "objectID": "about/MarcelaRobinson.html",
    "href": "about/MarcelaRobinson.html",
    "title": "Marcela Robinson",
    "section": "",
    "text": "##Instructions"
  },
  {
    "objectID": "about/MarcelaRobinson.html#educationwork-background",
    "href": "about/MarcelaRobinson.html#educationwork-background",
    "title": "Marcela Robinson",
    "section": "Education/Work Background",
    "text": "Education/Work Background\nI received my BA in Spanish and MBA from UMass. I have been working for the past seven years in higher education, as part of the operations team."
  },
  {
    "objectID": "about/MarcelaRobinson.html#r-experience",
    "href": "about/MarcelaRobinson.html#r-experience",
    "title": "Marcela Robinson",
    "section": "R experience",
    "text": "R experience\nI do not have any experience with R."
  },
  {
    "objectID": "about/MarcelaRobinson.html#research-interests",
    "href": "about/MarcelaRobinson.html#research-interests",
    "title": "Marcela Robinson",
    "section": "Research interests",
    "text": "Research interests\nI am interested in topics related to higher education such as diversity and inclusion in higher education, student accessibility to career services, intersectionality in career guidance, post graduate outcomes, particularly for underrepresented communities."
  },
  {
    "objectID": "about/MarcelaRobinson.html#hometown",
    "href": "about/MarcelaRobinson.html#hometown",
    "title": "Marcela Robinson",
    "section": "Hometown",
    "text": "Hometown\nPereira, COLOMBIA."
  },
  {
    "objectID": "about/MarcelaRobinson.html#hobbies",
    "href": "about/MarcelaRobinson.html#hobbies",
    "title": "Marcela Robinson",
    "section": "Hobbies",
    "text": "Hobbies\nI enjoy practicing yoga, spending time with my husband and 1 year old son, and cooking."
  },
  {
    "objectID": "about/MarcelaRobinson.html#fun-fact",
    "href": "about/MarcelaRobinson.html#fun-fact",
    "title": "Marcela Robinson",
    "section": "Fun fact",
    "text": "Fun fact\nI was born and raised in the coffee region of Colombia (located in the foothills of the Andes)."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Contributors",
    "section": "",
    "text": "Find out more about our DACSS students who contributed to the blog.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCristhian Barba Garzon\n\n\n\n\n\n\n\n\n\n\n\n\n\nMarcela Robinson\n\n\n\n\n\n\n\n\n\n\n\n\n\nTanmay Agrawal\n\n\n\n\n\n\n\n\n\n\n\n\n\nYour Name\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/challenge1_solutions.html",
    "href": "posts/challenge1_solutions.html",
    "title": "Challenge 1 Solution",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(readxl)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/challenge1_solutions.html#working-with-tabular-data",
    "href": "posts/challenge1_solutions.html#working-with-tabular-data",
    "title": "Challenge 1 Solution",
    "section": "Working with Tabular Data",
    "text": "Working with Tabular Data\nOur advanced datasets ( ⭐⭐⭐ and higher) are tabular data (i.e., tables) that are often published based on government sources or by other organizations. Tabular data is often made available in Excel format (.xls or .xlsx) and is formatted for ease of reading - but this can make it tricky to read into R and reshape into a usable dataset.\nReading in tabular data will follow the same general work flow or work process regardless of formatting differences. We will work through the steps in detail this week (and in future weeks as new datasets are introduced), but this is an outline of the basic process. Note that not every step is needed for every file.\n\nIdentify grouping variables and values to extract from the table\nIdentify formatting issues that need to be addressed or eliminated\nIdentify column issues to be addressed during data read-in\nChoose column names to allow pivoting or future analysis\nAddress issues in rows using filter (and stringr package)\nCreate or mutate new variables as required, using separate, pivot_longer, etc\n\n\nRailroad ⭐FAOSTAT ⭐⭐Wild Birds ⭐⭐⭐Railroad (xls) ⭐⭐⭐⭐\n\n\nIt is hard to get much information about the data source or contents from a .csv file - as compared to the formatted .xlsx version of the same data described below.\n\nRead the Data\n\n\nCode\nrailroad<-read_csv(\"_data/railroad_2012_clean_county.csv\")\n\n\nRows: 2930 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): state, county\ndbl (1): total_employees\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nrailroad\n\n\n\n\n  \n\n\n\nFrom inspection, we can that the three variables are named state, county, and total employees. Combined with the name of the file, this appears to be the aggregated data on the number of employees working for the railroad in each county 2012. We assume that the 2930 cases - which are counties embedded within states1 - consist only of counties where there are railroad employees?\n\n\nCode\nrailroad%>%\n  select(state)%>%\n  n_distinct(.)\n\n\n[1] 53\n\n\nCode\nrailroad%>%\n  select(state)%>%\n  distinct()\n\n\n\n\n  \n\n\n\nWith a few simple commands, we can confirm that there are 53 “states” represented in the data. To identify the additional non-state areas (probably District of Columbia, plus some combination of Puerto Rico and/or overseas addresses), we can print out a list of unique state names.\n\n1: We can identify case variables because both are character variables, which in tidy lingo are grouping variables not values.\n\n\n\nOnce again, a .csv file lacks any of the additional information that might be present in a published Excel table. So, we know the data are likely to be about birds, but will we be looking at individual pet birds, prices of bird breeds sold in stores, the average flock size of wild birds - who knows!\nThe FAOSTAT*.csv files have some additional information - the FAO - which a Google search reveals to be the Food and Agriculture Association of the United Nations publishes country-level data regularly in a database called FAOSTAT. So my best guess at this point is that we are going to be looking at country-level estaimtes of the number of birds that are raised for eggs and poultry, but we will see if this is right by inspecting the data.\n\nRead the Data\n\n\nCode\nbirds<-read_csv(\"_data/birds.csv\")\n\n\nRows: 30977 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (8): Domain Code, Domain, Area, Element, Item, Unit, Flag, Flag Description\ndbl (6): Area Code, Element Code, Item Code, Year Code, Year, Value\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nbirds\n\n\n\n\n  \n\n\n\nCode\nchickens<-read_csv(\"_data/FAOSTAT_egg_chicken.csv\")\n\n\nRows: 38170 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (8): Domain Code, Domain, Area, Element, Item, Unit, Flag, Flag Description\ndbl (6): Area Code, Element Code, Item Code, Year Code, Year, Value\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nchickens\n\n\n\n\n  \n\n\n\nIt is pretty difficult to get a handle on what data are being captured by any of the FAOSTAT* (including the birds.csv) data sets simply from a quick scan of the tibble after read in. It was easy with the railroad data, but now we are going to have to work harder to describe exactly what comprises a case in these data and what values are present for each case. We can see that there are 30,970 rows in the birds data (and 38,170 rows in the chickens) - but this might not mean that there are 30,970 (or 38,170) cases because we aren’t sure what constitutes a case at this point.\n\n\nWhat is a case?\nOne approach to figuring out what constitutes a case is to identify the value variables and assume that what is leftover are the grouping variables. Unfortunately, there are six double variables (from the column descriptions that are automatically returned), and it appears that most of them are not grouping variables. For example, the variable “Area Code” is a double - but doesn’t appear to be a value that varies across rows. Thus, it is a grouping variable rather than a true value in tidy nomenclature. Similar issues can be found with Year and “Item Code” - both appear to be grouping variables. Ironically, it is the variable called Value which appears to the sole value in the data set - but what is it the value of?\nAnother approach to identifying a case is to look for variation (or lack of variation) in just the first few cases of the tibble. (Think of this as the basis for a minimal reproducible example.) In the first few cases, the variables of the first 10 cases appear to be identical until we get to Year and Year Code (which appear to be identical to each other.) So it appears that Value is varying by country-year - but perhaps also by information in one of the other variables. It also appears that many of the doubles are just numeric codes, so lets drop those variables to simplify (I’m going to drop down to just showing the birds data for now.)\n\n\nCode\nbirds_sm<-birds%>%\n  select(-contains(\"Code\"))\nbirds_sm\n\n\n\n\n  \n\n\n\nCode\nchickens_sm<-chickens%>%\n  select(-contains(\"Code\"))\n\n\n\n\nVisual Summary of Data Set\nBefore we go doing detailed cross-tabs to figure out where there is variation, lets do a high level summary of the dataset to see if - for example - there are multiple values in the Element variable - or if we only have a dataset with records containing estimates of Chicken Stocks (from Element + Item.)\nTo get a better grasp of the data, lets do a quick skim or summary of the dataset and see if we can find out more about our data at a glance. I am using the dfSummary function from the summarytools package -one of the more attractive ways to quickly summarise a dataset. I am using a few options to allow it to render directly to html.\n\n\nCode\nprint(summarytools::dfSummary(birds_sm,\n                        varnumbers = FALSE,\n                        plain.ascii  = FALSE, \n                        style        = \"grid\", \n                        graph.magnif = 0.70, \n                        valid.col    = FALSE),\n      method = 'render',\n      table.classes = 'table-condensed')\n\n\n\n\nData Frame Summary\nbirds_sm\nDimensions: 30977 x 9\n  Duplicates: 0\n\n\n  \n    \n      Variable\n      Stats / Values\n      Freqs (% of Valid)\n      Graph\n      Missing\n    \n  \n  \n    \n      Domain\n[character]\n      1. Live Animals\n      30977(100.0%)\n      \n      0\n(0.0%)\n    \n    \n      Area\n[character]\n      1. Africa2. Asia3. Eastern Asia4. Egypt5. Europe6. France7. Greece8. Myanmar9. Northern Africa10. South-eastern Asia[ 238 others ]\n      290(0.9%)290(0.9%)290(0.9%)290(0.9%)290(0.9%)290(0.9%)290(0.9%)290(0.9%)290(0.9%)290(0.9%)28077(90.6%)\n      \n      0\n(0.0%)\n    \n    \n      Element\n[character]\n      1. Stocks\n      30977(100.0%)\n      \n      0\n(0.0%)\n    \n    \n      Item\n[character]\n      1. Chickens2. Ducks3. Geese and guinea fowls4. Pigeons, other birds5. Turkeys\n      13074(42.2%)6909(22.3%)4136(13.4%)1165(3.8%)5693(18.4%)\n      \n      0\n(0.0%)\n    \n    \n      Year\n[numeric]\n      Mean (sd) : 1990.6 (16.7)min ≤ med ≤ max:1961 ≤ 1992 ≤ 2018IQR (CV) : 29 (0)\n      58 distinct values\n      \n      0\n(0.0%)\n    \n    \n      Unit\n[character]\n      1. 1000 Head\n      30977(100.0%)\n      \n      0\n(0.0%)\n    \n    \n      Value\n[numeric]\n      Mean (sd) : 99410.6 (720611.4)min ≤ med ≤ max:0 ≤ 1800 ≤ 23707134IQR (CV) : 15233 (7.2)\n      11495 distinct values\n      \n      1036\n(3.3%)\n    \n    \n      Flag\n[character]\n      1. *2. A3. F4. Im5. M\n      1494(7.4%)6488(32.1%)10007(49.5%)1213(6.0%)1002(5.0%)\n      \n      10773\n(34.8%)\n    \n    \n      Flag Description\n[character]\n      1. Aggregate, may include of2. Data not available3. FAO data based on imputat4. FAO estimate5. Official data6. Unofficial figure\n      6488(20.9%)1002(3.2%)1213(3.9%)10007(32.3%)10773(34.8%)1494(4.8%)\n      \n      0\n(0.0%)\n    \n  \n\nGenerated by summarytools 1.0.1 (R version 4.2.1)2022-12-26\n\n\n\nWe can also use the base R summary function.\n\n\nCode\nsummary(birds_sm)\n\n\n    Domain              Area             Element              Item          \n Length:30977       Length:30977       Length:30977       Length:30977      \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n      Year          Unit               Value              Flag          \n Min.   :1961   Length:30977       Min.   :       0   Length:30977      \n 1st Qu.:1976   Class :character   1st Qu.:     171   Class :character  \n Median :1992   Mode  :character   Median :    1800   Mode  :character  \n Mean   :1991                      Mean   :   99411                     \n 3rd Qu.:2005                      3rd Qu.:   15404                     \n Max.   :2018                      Max.   :23707134                     \n                                   NA's   :1036                         \n Flag Description  \n Length:30977      \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n                   \n\n\nFinally - we have a much better grasp on what is going on. First, we know that all records in this data set are of the number of Live Animal Stocks (Domain + Element), with the value expressed as 1000 heads (Unit). These three variables are grouping variables but DO NOT vary in this particular data extract - but are probably used to create data extracts from the larger FAOSTAT database.. To see if we are correct, we will have to checkout the same fields in the chickens data below.\nSecond, we can now guess that a case consists of a country-year-animal record - as captured in the variables Area, Year and Item, respectively - estimate of the number of live animals (Value.) ALso, as a side note, it appears that the estimated number of animals may have a long right-hand tail - just looking at the mini-histogram. So we can now say that we have estimates of the stock of five different types of poultry (Chickens, Ducks, Geese and guinea fowls, Turkeys, and Pigeons/Others) in 248 areas (countries??) for 58 years between 1961-2018.\nThe only minor concern is that we are still not entirely sure what information is being captured in the Flag (and matching Flag Description) variable. It appears unlikely that there is more than one estimate per country-year-animal case (see the summary of Area where all countries have 290 observations.) An assumption of one type of estimate (the content of Flag Description) per year is also consistent with the histogram of Year, which is pretty consistent although more countries were clearly added later in the series and data collection is not complete for the most recent time period.\nWe can dig a bit more, and find the description of the Flag field on the FAOSTAT website.. Sure enough, this confirms that the flags correspond to what type of estimate is being used (e.g., official data vs an estimate by FAOSTAT or imputed data.)\nWe can also confirm that NOT all cases are countries, as there is a Flag value, A, described as aggregated data. A quick inspection of the areas using this flag confirm that all of the “countries” are actually regional aggregations, and should be filtered out of the dataset as they are not the same “type” of case as a country-level case. To fix these data into true tidy format, we would need to filter out the aggregates, then merge on the country group definitions from FAOSTAT to create new country-group or regional variables that could be used to recreate aggregated estimates with dplyr.\n\n\nCode\nbirds_sm%>%\n  filter(Flag==\"A\")%>%\n  group_by(Area)%>%\n  summarize(n=n())\n\n\n\n\n  \n\n\n\n\n\nFAOstat*.csv\nLets take a quick look at our chickens data to see if it follows the same basic pattern as the birds data. Sure enough, it looks like we have a different domain (livestock products) but that the cases remain similar country-year-product, with three slightly different estimates related to egg-laying (instead of the five types of poultry.)\n\n\nCode\nprint(summarytools::dfSummary(chickens_sm,\n                        varnumbers = FALSE,\n                        plain.ascii  = FALSE, \n                        style        = \"grid\", \n                        graph.magnif = 0.70, \n                        valid.col    = FALSE),\n      method = 'render',\n      table.classes = 'table-condensed')\n\n\n\n\nData Frame Summary\nchickens_sm\nDimensions: 38170 x 9\n  Duplicates: 0\n\n\n  \n    \n      Variable\n      Stats / Values\n      Freqs (% of Valid)\n      Graph\n      Missing\n    \n  \n  \n    \n      Domain\n[character]\n      1. Livestock Primary\n      38170(100.0%)\n      \n      0\n(0.0%)\n    \n    \n      Area\n[character]\n      1. Afghanistan2. Africa3. Albania4. Algeria5. American Samoa6. Americas7. Angola8. Antigua and Barbuda9. Argentina10. Asia[ 235 others ]\n      174(0.5%)174(0.5%)174(0.5%)174(0.5%)174(0.5%)174(0.5%)174(0.5%)174(0.5%)174(0.5%)174(0.5%)36430(95.4%)\n      \n      0\n(0.0%)\n    \n    \n      Element\n[character]\n      1. Laying2. Production3. Yield\n      12679(33.2%)12840(33.6%)12651(33.1%)\n      \n      0\n(0.0%)\n    \n    \n      Item\n[character]\n      1. Eggs, hen, in shell\n      38170(100.0%)\n      \n      0\n(0.0%)\n    \n    \n      Year\n[numeric]\n      Mean (sd) : 1990.5 (16.7)min ≤ med ≤ max:1961 ≤ 1991 ≤ 2018IQR (CV) : 29 (0)\n      58 distinct values\n      \n      0\n(0.0%)\n    \n    \n      Unit\n[character]\n      1. 1000 Head2. 100mg/An3. tonnes\n      12679(33.2%)12651(33.1%)12840(33.6%)\n      \n      0\n(0.0%)\n    \n    \n      Value\n[numeric]\n      Mean (sd) : 291341.2 (2232761)min ≤ med ≤ max:1 ≤ 31996 ≤ 76769955IQR (CV) : 91235.8 (7.7)\n      21325 distinct values\n      \n      40\n(0.1%)\n    \n    \n      Flag\n[character]\n      1. *2. A3. F4. Fc5. Im6. M\n      1435(4.7%)3186(10.4%)10538(34.4%)13344(43.6%)2079(6.8%)40(0.1%)\n      \n      7548\n(19.8%)\n    \n    \n      Flag Description\n[character]\n      1. Aggregate, may include of2. Calculated data3. Data not available4. FAO data based on imputat5. FAO estimate6. Official data7. Unofficial figure\n      3186(8.3%)13344(35.0%)40(0.1%)2079(5.4%)10538(27.6%)7548(19.8%)1435(3.8%)\n      \n      0\n(0.0%)\n    \n  \n\nGenerated by summarytools 1.0.1 (R version 4.2.1)2022-12-26\n\n\n\n\n\n\nThe “wild_bird_data” sheet is in Excel format (.xlsx) instead of the .csv format of the earlier data sets. In theory, it should be no harder to read in an Excel worksheet (or even workbook) as compared to a .csv file - there is a package called read_xl that is part of the tidyverse that easily reads in excel files.\nHowever, in practice, most people use Excel sheets as a publication format - not a way to store data, so there is almost always a ton of “junk” in the file that is NOT part of the data table that we want to read in. Sometimes the additional “junk” is incredibly useful - it might include table notes or information about data sources. However, we still need a systematic way to identify this junk and get rid of it during the data reading step.\nFor example, lets see what happens here if we just read in the wild bird data straight from excel.\n\n\nCode\nwildbirds<-read_excel(\"_data/wild_bird_data.xlsx\")\nwildbirds\n\n\n\n\n  \n\n\n\nHm, this doesn’t seem quite right. It is clear that the first “case” has information in it that looks more like variable labels. Lets take a quick look at the raw data.\n\n\n\nWild Bird Excel File\n\n\nSure enough the Excel file first row does contain additional information, a pointer to the article that this data was drawn from, and a quick Google reveals the article is [Nee, S., Read, A., Greenwood, J. et al. The relationship between abundance and body size in British birds. Nature 351, 312–313 (1991)] (https://www.nature.com/articles/351312a0)\n\nSkipping a row\nWe could try to manually adjust things - remove the first row, change the column names, and then change the column types. But this is both a lot of work, and not really a best practice for data management. Lets instead re-read the data in with the skip option from read_excel, and see if it fixes all of our problems!\n\n\nCode\nwildbirds <- read_excel(\"_data/wild_bird_data.xlsx\",\n                        skip = 1)\nwildbirds\n\n\n\n\n  \n\n\n\nThis now looks great! Both variables are numeric, and now they correctly show up as double or (). The variable names might be a bit tough to work with, though, so it can be easier to assign new column names on the read in - and then manually adjust axis labels, etc once you are working on your publication-quality graphs.\nNote that I skip two rows this time, and apply my own column names.\n\n\nCode\nwildbirds <- read_excel(\"_data/wild_bird_data.xlsx\",\n                        skip = 2, \n                        col_names = c(\"weight\", \"pop_size\"))\nwildbirds\n\n\n\n\n  \n\n\n\n\n\n\nThe railroad data set is our most challenging data to read in this week, but is (by comparison) a fairly straightforward formatted table published by the Railroad Retirement Board. The value variable is a count of the number of employees in each county and state combination. \nLooking at the excel file, we can see that there are only a few issues: 1. There are three rows at the top of the sheet that are not needed 2. There are blank columns that are not needed. 3. There are Total rows for each state that are not needed\n\nSkipping title rows\nFor the first issue, we use the “skip” option on read_excel from the readxl package to skip the rows at the top.\n\n\nCode\nread_excel(\"_data/StateCounty2012.xls\",\n                     skip = 3)\n\n\nNew names:\n• `` -> `...2`\n• `` -> `...4`\n\n\n\n\n  \n\n\n\n\n\nRemoving empty columns\nFor the second issue, I name the blank columns “delete” to make is easy to remove the unwanted columns. I then use select (with the ! sign to designate the complement or NOT) to select columns we wish to keep in the dataset - the rest are removed. Note that I skip 4 rows this time as I do not need the original header row.\nThere are other approaches you could use for this task (e.g., remove all columns that have no valid volues), but hard coding of variable names and types during data read in is not considered a violation of best practices and - if used strategically - can often make later data cleaning much easier.\n\n\nCode\nread_excel(\"_data/StateCounty2012.xls\",\n                     skip = 4,\n                     col_names= c(\"State\", \"delete\", \"County\", \"delete\", \"Employees\"))%>%\n  select(!contains(\"delete\"))\n\n\nNew names:\n• `delete` -> `delete...2`\n• `delete` -> `delete...4`\n\n\n\n\n  \n\n\n\n\n\nFiltering “total” rows\nFor the third issue, we are going to use filter to identify (and drop the rows that have the word “Total” in the State column). str_detect can be used to find specific rows within a column that have the designated “pattern”, while the “!” designates the complement of the selected rows (i.e., those without the “pattern” we are searching for.)\nThe str_detect command is from the stringr package, and is a powerful and easy to use implementation of grep and regex in the tidyverse - the base R functions (grep, gsub, etc) are classic but far more difficult to use, particularly for those not in practice. Be sure to explore the stringr package on your own.\n\n\nCode\nrailroad<-read_excel(\"_data/StateCounty2012.xls\",\n                     skip = 4,\n                     col_names= c(\"State\", \"delete\", \"County\", \"delete\", \"Employees\"))%>%\n  select(!contains(\"delete\"))%>%\n  filter(!str_detect(State, \"Total\"))\n\n\nNew names:\n• `delete` -> `delete...2`\n• `delete` -> `delete...4`\n\n\nCode\nrailroad\n\n\n\n\n  \n\n\n\n\n\nRemove any table notes\nTables often have notes in the last few table rows. You can check table limits and use this information during data read-in to not read the notes by setting the n-max option at the total number of rows to read, or less commonly, the range option to specify the spreadsheet range in standard excel naming (e.g., “B4:R142”). If you didn’t handle this on read in, you can use the tail command to check for notes and either tail or head to keep only the rows that you need.\n\n\nCode\ntail(railroad, 10)\n\n\n\n\n  \n\n\n\nCode\n#remove the last two observations\nrailroad <- head(railroad, -2)\ntail(railroad, 10)\n\n\n\n\n  \n\n\n\n\n\nThe range approach\nWe can manually specify the range of cells we want to read in using the range argument. To do so, you’ll need to open the file up in Excel (or a similar program) and figure this out on your own.\n\n\nCode\nrailroad_new <- read_excel(\"_data/StateCounty2012.xls\",\n                     range = \"B4:F2990\",\n                     col_names= c(\"State\", \"delete\", \"County\", \"delete\", \"Employees\"))%>%\n  select(!contains(\"delete\"))%>%\n  filter(!str_detect(State, \"Total\"))\n\n\nNew names:\n• `delete` -> `delete...2`\n• `delete` -> `delete...4`\n\n\nCode\nrailroad_new\n\n\n\n\n  \n\n\n\nCode\ntail(railroad_new,10)\n\n\n\n\n  \n\n\n\n\n\nConfirm cases\nAnd that is all it takes! The data are now ready for analysis. Lets see if we get the same number of unique states that were in the cleaned data in exercise 1.\n\n\nCode\nrailroad%>%\n  select(State)%>%\n  n_distinct(.)\n\n\n[1] 54\n\n\nCode\nrailroad%>%\n  select(State)%>%\n  distinct()\n\n\n\n\n  \n\n\n\nOh my goodness! It seems that we have an additional “State” - it looks like Canada is in the full excel data and not the tidy data. This is one example of why it is good practice to always work from the original data source!"
  },
  {
    "objectID": "posts/example-data_import.html",
    "href": "posts/example-data_import.html",
    "title": "Data Import",
    "section": "",
    "text": "Today, we’re going to read in three versions of the poultry_tidy data. These data are available in the _data in the course blog repo.\nWe will specifically read in 3 data files:\n- poultry_tidy.csv\n- poultry_tidy.xlsx\n- poultry_tidy.RData\nThese are the “clean” versions of the raw data files.\nTo run this file, all 3 datasets should be in the same directory on your computer.\nI also use the here package to manage relative directories."
  },
  {
    "objectID": "posts/example-data_import.html#getting-started",
    "href": "posts/example-data_import.html#getting-started",
    "title": "Data Import",
    "section": "Getting Started",
    "text": "Getting Started\nTo begin, we need to load two packages: readr and readxl, which contain very useful functions for reading in data to `R.\n\nlibrary(readr)\nlibrary(readxl)\n\nIf you’re unsure whether or not you have these packages installed, you can run the following command:\n\ninstalled.packages()\n\nWe’re now ready to get started reading in actual datasets."
  },
  {
    "objectID": "posts/example-data_import.html#reading-in-delimited-text-files",
    "href": "posts/example-data_import.html#reading-in-delimited-text-files",
    "title": "Data Import",
    "section": "Reading in delimited text files",
    "text": "Reading in delimited text files\n.csv is a common type of delimited text file. .csv stands for comma-separated value. This means that commas separate cells from one another.\nR has a base read.csv() function. However, it comes with a couple of downsides - namely that it imports data as a dataframe rather than a tibble. So we will be using the function read_csv() from the readr package. In addition to importing data as a tibble, it also does a much better job guessing data types.\nread_csv() is essentially a wrapper function (a function that calls another function) around the more general read_delim() function. Also see read_tsv() for tab-separated values.\n\n?read_delim\n\nLet’s look at the data files available for us to read in:\n\nlist.files(here(\"posts\",\"_data\"))\n\n [1] \"AB_NYC_2019.csv\"                                                                                 \n [2] \"abc_poll_2021.csv\"                                                                               \n [3] \"ActiveDuty_MaritalStatus.xls\"                                                                    \n [4] \"animal_weight.csv\"                                                                               \n [5] \"australian_marriage_law_postal_survey_2017_-_response_final.xls\"                                 \n [6] \"australian_marriage_tidy.csv\"                                                                    \n [7] \"birds.csv\"                                                                                       \n [8] \"cereal.csv\"                                                                                      \n [9] \"cwc.csv\"                                                                                         \n[10] \"Data_Extract_From_World_Development_Indicators.xlsx\"                                             \n[11] \"Data_Extract_FromWorld Development Indicators.xlsx\"                                              \n[12] \"debt_in_trillions.xlsx\"                                                                          \n[13] \"eggs_tidy.csv\"                                                                                   \n[14] \"FAOSTAT_cattle_dairy.csv\"                                                                        \n[15] \"FAOSTAT_country_groups.csv\"                                                                      \n[16] \"FAOSTAT_egg_chicken.csv\"                                                                         \n[17] \"FAOSTAT_livestock.csv\"                                                                           \n[18] \"FedFundsRate.csv\"                                                                                \n[19] \"FRBNY-SCE-Public-Microdata-Complete-13-16.xlsx\"                                                  \n[20] \"hotel_bookings.csv\"                                                                              \n[21] \"organiceggpoultry.xls\"                                                                           \n[22] \"poultry_tidy.csv\"                                                                                \n[23] \"poultry_tidy.RData\"                                                                              \n[24] \"poultry_tidy.xlsx\"                                                                               \n[25] \"Public_School_Characteristics_2017-18.csv\"                                                       \n[26] \"railroad_2012_clean_county.csv\"                                                                  \n[27] \"sce-labor-chart-data-public.xlsx\"                                                                \n[28] \"snl_actors.csv\"                                                                                  \n[29] \"snl_casts.csv\"                                                                                   \n[30] \"snl_seasons.csv\"                                                                                 \n[31] \"starwars1.RData\"                                                                                 \n[32] \"StateCounty2012.xls\"                                                                             \n[33] \"test_objs.RData\"                                                                                 \n[34] \"Total_cost_for_top_15_pathogens_2018.xlsx\"                                                       \n[35] \"USA Households by Total Money Income, Race, and Hispanic Origin of Householder 1967 to 2019.xlsx\"\n[36] \"wild_bird_data.xlsx\"                                                                             \n\n\nThere’s a lot of data files there, but we are going to import the poultry_tidy.csv file. Doing so is very simple using read_csv():\n\npoultry_from_csv <- read_csv(here(\"posts\",\"_data\",\"poultry_tidy.csv\"))\n\nRows: 600 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): Product, Month\ndbl (2): Year, Price_Dollar\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nLet’s take a look at our dataset (to view the tibble, running the name of the object will print it to the console):\n\npoultry_from_csv\n\n\n\n  \n\n\n\nIt worked great! The data is all there. To inspect the data types for each of the four columns in poultry_from_csv, we can use spec() or typeof():\nNote: we can also avoid the setwd() issue by modifying the name of the file to include the file path:\n\npoultry_from_csv <- read_csv(here(\"posts\",\"_data\",\"poultry_tidy.csv\"))\n\n\nspec(poultry_from_csv) # use the spec() function to check the data type for your columns\n\ncols(\n  Product = col_character(),\n  Year = col_double(),\n  Month = col_character(),\n  Price_Dollar = col_double()\n)\n\n# can also use typeof() function on individual columns\ntypeof(poultry_from_csv$Product)\n\n[1] \"character\"\n\ntypeof(poultry_from_csv$Year)\n\n[1] \"double\"\n\ntypeof(poultry_from_csv$Month)\n\n[1] \"character\"\n\ntypeof(poultry_from_csv$Price_Dollar)\n\n[1] \"double\"\n\n\nSee this R section below for some more info on read_delim():\n\n# read_delim() has a number of optional arguments\nargs(read_delim)\n\nfunction (file, delim = NULL, quote = \"\\\"\", escape_backslash = FALSE, \n    escape_double = TRUE, col_names = TRUE, col_types = NULL, \n    col_select = NULL, id = NULL, locale = default_locale(), \n    na = c(\"\", \"NA\"), quoted_na = TRUE, comment = \"\", trim_ws = FALSE, \n    skip = 0, n_max = Inf, guess_max = min(1000, n_max), name_repair = \"unique\", \n    num_threads = readr_threads(), progress = show_progress(), \n    show_col_types = should_show_types(), skip_empty_rows = TRUE, \n    lazy = should_read_lazy()) \nNULL\n\n# there's too many to list here, so we will just go over a few\n# run ?read_delim() to learn more\n# 1) delim - text delimiter.\n# default is NULL and read_delim() guesses delimiter\n#\n# 2) quote - symbol telling R when to quote a string\n# default is \"\\\"\"\n# below comes from R documentation on quotes\n# https://stat.ethz.ch/R-manual/R-devel/library/base/html/Quotes.html\n# identical() is a function that returns TRUE if two objects are equal\nidentical(1+4, 3+2)\n\n[1] TRUE\n\nidentical('\"It\\'s alive!\", he screamed.',\n          \"\\\"It's alive!\\\", he screamed.\") # same\n\n[1] TRUE\n\n#\n# 3) escape_backlash\n# use backlash to escape special characters?\n# default = FALSE\n#\n# 4) col_names\n# can be TRUE (default), meaning that R reads in the first row of values as column names\n# can FALSE - R creates column names (x1 x2 etc)\n# OR can be a character vector of custom column names\npoultry_custom_cols <- read_csv(\"_data/poultry_tidy.csv\",\n                                col_names = c(\"prod\",\"yr\",\"mo\",\"$\"),\n                                skip = 1) # need this to skip the file's column names\n\nRows: 600 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): prod, mo\ndbl (2): yr, $\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\npoultry_custom_cols\n\n\n\n  \n\n\npoultry_custom_cols$`$` # note the backticks around the $ sign\n\n  [1] 2.38500 2.38500 2.38500 2.38500 2.38500 2.38500 2.38500 2.38500 2.38500\n [10] 2.38500 2.38500 2.38500 7.03750 7.03750 7.03750 7.03750 7.03750 7.03750\n [19] 7.03750 7.03750 7.03750 7.03750 7.03750 7.03750 3.90500 3.90500 3.90500\n [28] 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500\n [37] 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500\n [46] 2.03500 2.03500 2.03500 2.16250 2.16250 2.16250 2.16250 2.16250 2.16250\n [55] 2.16250 2.16250 2.16250 2.16250 2.16250 2.16250 2.35000 2.38500 2.38500\n [64] 2.38500 2.38500 2.38500 2.38500 2.38500 2.38500 2.38500 2.38500 2.38500\n [73] 6.37500 7.00000 7.00000 7.00000 7.00000 7.00000 7.00000 7.00000 7.00000\n [82] 7.00000 7.03750 7.03750 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500\n [91] 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500 2.03500 2.03500 2.03500\n[100] 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500\n[109] 2.15000 2.15000 2.15000 2.15000 2.15000 2.15000 2.15000 2.16250 2.16250\n[118] 2.16250 2.16250 2.16250 2.35000 2.35000 2.35000 2.35000 2.35000 2.35000\n[127] 2.35000 2.35000 2.35000 2.35000 2.35000 2.35000 6.37500 6.37500 6.37500\n[136] 6.37500 6.37500 6.37500 6.37500 6.37500 6.37500 6.37500 6.37500 6.37500\n[145] 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500\n[154] 3.90500 3.90500 3.90500 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500\n[163] 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500 2.15000 2.15000 2.15000\n[172] 2.15000 2.15000 2.15000 2.15000 2.15000 2.15000 2.15000 2.15000 2.15000\n[181] 2.48000 2.48000 2.48000 2.41500 2.35000 2.35000 2.41500 2.35000 2.35000\n[190] 2.35000 2.35000 2.35000 6.45500 6.45500 6.45500 6.45500 6.45500 6.45500\n[199] 6.45500 6.42300 6.37500 6.37500 6.37500 6.37500 3.90500 3.90500 3.90500\n[208] 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500\n[217] 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500\n[226] 2.03500 2.03500 2.03500 2.22000 2.22000 2.22000 2.22000 2.22000 2.22000\n[235] 2.22000 2.19200 2.15000 2.15000 2.15000 2.15000 2.48000 2.48000 2.48000\n[244] 2.48000 2.48000 2.48000 2.48000 2.48000 2.48000 2.48000 2.48000 2.48000\n[253] 6.45500 6.45500 6.45500 6.45500 6.45500 6.45500 6.45500 6.45500 6.45500\n[262] 6.45500 6.45500 6.45500 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500\n[271] 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500 2.03500 2.03500 2.03500\n[280] 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500\n[289] 2.22000 2.22000 2.22000 2.22000 2.22000 2.22000 2.22000 2.22000 2.22000\n[298] 2.22000 2.22000 2.22000 2.20500 2.20500 2.20500 2.20500 2.20500 2.48000\n[307] 2.48000 2.48000 2.48000 2.48000 2.48000 2.48000 6.45500 6.45500 6.45500\n[316] 6.45500 6.45500 6.45500 6.45500 6.45500 6.45500 6.45500 6.45500 6.45500\n[325] 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500\n[334] 3.90500 3.90500 3.90500 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500\n[343] 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500 2.22000 2.22000 2.22000\n[352] 2.22000 2.22000 2.22000 2.22000 2.22000 2.22000 2.22000 2.22000 2.22000\n[361] 2.20500 2.20500 2.20500 2.20500 2.20500 2.20500 2.20500 2.20500 2.20500\n[370] 2.20500 2.20500 2.20500 6.45500 6.45500 6.45500 6.45500 6.45500 6.45500\n[379] 6.45500 6.45500 6.45500 6.45500 6.45500 6.45500 3.90500 3.90500 3.90500\n[388] 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500\n[397] 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500\n[406] 2.03500 2.03500 2.03500 2.22000 2.22000 2.22000 2.22000 2.22000 2.22000\n[415] 2.22000 2.22000 2.22000 2.22000 2.22000 2.22000 2.17000 2.17000 2.19625\n[424] 2.20500 2.20500 2.20500 2.20500 2.20500 2.20500 2.20500 2.20500 2.20500\n[433] 6.45500 6.45500 6.45500 6.45500 6.45500 6.45500 6.45500 6.45500 6.45500\n[442] 6.45500 6.45500 6.45500 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500\n[451] 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500 2.03500 2.03500 2.03500\n[460] 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500\n[469] 2.22000 2.22000 2.22000 2.22000 2.22000 2.22000 2.22000 2.22000 2.22000\n[478] 2.22000 2.22000 2.22000 2.17000 2.17000 2.17000 2.17000 2.17000 2.17000\n[487] 2.17000 2.17000 2.17000 2.17000 2.17000 2.17000 6.44000 6.45500 6.45500\n[496] 6.45500 6.45500 6.45500 6.45500 6.45500 6.45500 6.45500 6.45500 6.45500\n[505] 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500\n[514] 3.90500 3.90500 3.90500 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500\n[523] 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500 2.13000 2.22000 2.22000\n[532] 2.22000 2.22000 2.22000 2.22000 2.22000 2.22000 2.22000 2.22000 2.22000\n[541] 1.97500 1.97500 2.09000 2.12000 2.14500 2.16375 2.17000 2.17000 2.17000\n[550] 2.17000 2.17000 2.17000 6.45500 6.42500 6.42500 6.42500 6.42500 6.41000\n[559] 6.42500 6.42500 6.42500 6.42500 6.42500 6.42500      NA      NA      NA\n[568]      NA      NA      NA 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500\n[577] 1.93500 1.93500 1.93500 1.93500 1.93500 2.01875 2.03500 2.03500 2.03500\n[586] 2.03500 2.03500 2.03500      NA 2.03000 2.03000 2.03000 2.03000 2.00375\n[595] 1.99500 1.99500 1.99500 1.99500 1.99500 1.99500\n\n# $ is a \"special symbol\" in R, because it is an operator used for indexing\n# $ is technically an illegal column name, but we can still use it with ``\n# same goes for column names consisting of numbers or other symbols, etc.\n#\n# 5) col_types\n# default=NULL\n# if NULL R guesses data type from first 1000 rows\n# can also specify manually (but be careful)\n# see ?read_delim and scroll to col_types for details\n#\n# 6) skip\n# number of lines to skip\n# default=0\n# can be very useful with messy data files\n#\n# 7) n_max\n# maximum number of lines to read\n# default=Inf\n#\n#"
  },
  {
    "objectID": "posts/example-data_import.html#read-in-.xls.xlsx-files",
    "href": "posts/example-data_import.html#read-in-.xls.xlsx-files",
    "title": "Data Import",
    "section": "Read in .xls/.xlsx files",
    "text": "Read in .xls/.xlsx files\n.xls and .xlsx are files created in Microsoft Excel. There are separate functions read_xls() and read_xlsx(), but I find it’s best to use the wrapper function read_excel(). This will automatically call the correct function and avoid an error from accidentally mis-specifying the file type.\nSee below for what happens if we call the wrong function for the file type:\n\n# the try() function will try to run the code\n# see tryCatch() for more error handling \n# this code doesn't work because it tries to read the wrong file type\ntry(read_xls(here(\"posts\",\"_data\",\"poultry_tidy.xlsx\")))\n\nError : \n  filepath: /Users/seanconway/Github/601_Winter_2022-2023/posts/_data/poultry_tidy.xlsx\n  libxls error: Unable to open file\n\n\nThe code below works just fine, however:\n\n# this code works \npoultry_from_excel <- try(read_excel(here(\"posts\",\"_data\",\"poultry_tidy.xlsx\"),\n                                     skip = 5,\n                                     col_names = c(\"prod\",\"year\",\"month\",\"price\"))) \npoultry_from_excel \n\n\n\n  \n\n\n\nLet’s take a look at this tibble:\n\n# examining our tibble\nhead(poultry_from_excel) # view the first several rows\n\n\n\n  \n\n\ncolnames(poultry_from_excel) # print column names\n\n[1] \"prod\"  \"year\"  \"month\" \"price\"\n\ndplyr::glimpse(poultry_from_excel) # tidy little summary of it\n\nRows: 596\nColumns: 4\n$ prod  <chr> \"Whole\", \"Whole\", \"Whole\", \"Whole\", \"Whole\", \"Whole\", \"Whole\", \"…\n$ year  <dbl> 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013…\n$ month <chr> \"May\", \"June\", \"July\", \"August\", \"September\", \"October\", \"Novemb…\n$ price <dbl> 2.3850, 2.3850, 2.3850, 2.3850, 2.3850, 2.3850, 2.3850, 2.3850, …\n\n# the package::function() syntax is only necessary if the package isn't loaded\n\nFunction documentation:\n\n# to view function documentation\n?read_excel\n\n# optional arguments\n# 1) sheet=NULL\n# number of the sheet to read in\n# by default it reads the first sheet\n\n# 2) range=NULL\n# range of cells to read in\n# uses the cellranger package to work with specific cells in Excel files\n# for more, see the cellranger package\n# https://cran.r-project.org/web/packages/cellranger/index.html\n\n# 3) col_names=TRUE\n# how to get column names (works the same as read_delim())\n\n# 4) col_types=NULL\n# types of data in columns (works the same as read_delim())\n\n# 5) skip = 0\n# number of lines to skip (works the same as read_delim())\n\n# 6) n_max=Inf\n# max lines to read (works the same as read_delim())"
  },
  {
    "objectID": "posts/example-data_import.html#reading-in-.rdata-files",
    "href": "posts/example-data_import.html#reading-in-.rdata-files",
    "title": "Data Import",
    "section": "Reading in .RData Files",
    "text": "Reading in .RData Files\nReading .RData is less commonly needed, but it’s still important to know about. .RData is a file type exclusively associated with R. It’s commonly used when someone has performed operations with data and saved the results to give to collaborators.\nWe can use the load() function to load R objects into our R environment from a file:\n\n# running the load() function on the data file name will load the objects into your R environment\nload(here(\"posts\",\"_data\",\"poultry_tidy.RData\"))\npoultry_tidy\n\n\n\n  \n\n\n# there's now a poultry_tidy object in our R environment\n\nNote that we do not assign the data file to an object. Rather, it comes in as an object based on whatever the previous user named it as. If we try to assign it as an object, the object will only have the name of the data file, rather than the data itself:\n\n# note that this operation shouldn't include any variable assignment\ntest_dat <- load(here(\"posts\",\"_data\",\"poultry_tidy.RData\"))\ntest_dat # now it contains the object name, not the object itself\n\n[1] \"poultry_tidy\"\n\n\nYou can also save any number of R objects to a .RData file using the save() function:\n\na <- rnorm(1000)\nb <- matrix(runif(100),nrow=50,ncol=2)\nc <- as_tibble(mtcars)\nsave(a,b,c,file=here(\"posts\",\"_data\",\"test_objs.RData\"))\n# there is now a test_objs.RData file in my working directory: \nlist.files(here(\"posts\",\"_data/\"))\n\n [1] \"AB_NYC_2019.csv\"                                                                                 \n [2] \"abc_poll_2021.csv\"                                                                               \n [3] \"ActiveDuty_MaritalStatus.xls\"                                                                    \n [4] \"animal_weight.csv\"                                                                               \n [5] \"australian_marriage_law_postal_survey_2017_-_response_final.xls\"                                 \n [6] \"australian_marriage_tidy.csv\"                                                                    \n [7] \"birds.csv\"                                                                                       \n [8] \"cereal.csv\"                                                                                      \n [9] \"cwc.csv\"                                                                                         \n[10] \"Data_Extract_From_World_Development_Indicators.xlsx\"                                             \n[11] \"Data_Extract_FromWorld Development Indicators.xlsx\"                                              \n[12] \"debt_in_trillions.xlsx\"                                                                          \n[13] \"eggs_tidy.csv\"                                                                                   \n[14] \"FAOSTAT_cattle_dairy.csv\"                                                                        \n[15] \"FAOSTAT_country_groups.csv\"                                                                      \n[16] \"FAOSTAT_egg_chicken.csv\"                                                                         \n[17] \"FAOSTAT_livestock.csv\"                                                                           \n[18] \"FedFundsRate.csv\"                                                                                \n[19] \"FRBNY-SCE-Public-Microdata-Complete-13-16.xlsx\"                                                  \n[20] \"hotel_bookings.csv\"                                                                              \n[21] \"organiceggpoultry.xls\"                                                                           \n[22] \"poultry_tidy.csv\"                                                                                \n[23] \"poultry_tidy.RData\"                                                                              \n[24] \"poultry_tidy.xlsx\"                                                                               \n[25] \"Public_School_Characteristics_2017-18.csv\"                                                       \n[26] \"railroad_2012_clean_county.csv\"                                                                  \n[27] \"sce-labor-chart-data-public.xlsx\"                                                                \n[28] \"snl_actors.csv\"                                                                                  \n[29] \"snl_casts.csv\"                                                                                   \n[30] \"snl_seasons.csv\"                                                                                 \n[31] \"starwars1.RData\"                                                                                 \n[32] \"StateCounty2012.xls\"                                                                             \n[33] \"test_objs.RData\"                                                                                 \n[34] \"Total_cost_for_top_15_pathogens_2018.xlsx\"                                                       \n[35] \"USA Households by Total Money Income, Race, and Hispanic Origin of Householder 1967 to 2019.xlsx\"\n[36] \"wild_bird_data.xlsx\"                                                                             \n\n\nLet’s remove these objects from our R environment and re-load them from the file we saved:\n\n# remove objects from environment\nrm(list=c(\"a\",\"b\",\"c\"))\n\n# now they're back! (If you save them)\ntry(load(here(\"posts\",\"_data\",\"test_objs.RData\")))"
  },
  {
    "objectID": "posts/example-data_import.html#conclusion",
    "href": "posts/example-data_import.html#conclusion",
    "title": "Data Import",
    "section": "Conclusion",
    "text": "Conclusion\nYou now know a little bit about how to read in some common data types. Note that these aren’t the only types of data you’ll encounter, but they are by far the most common ones."
  },
  {
    "objectID": "posts/challenge2_instructions.html",
    "href": "posts/challenge2_instructions.html",
    "title": "Challenge 2 Instructions",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge2_instructions.html#challenge-overview",
    "href": "posts/challenge2_instructions.html#challenge-overview",
    "title": "Challenge 2 Instructions",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to\n\nread in a data set, and describe the data using both words and any supporting information (e.g., tables, etc)\nprovide summary statistics for different interesting groups within the data, and interpret those statistics"
  },
  {
    "objectID": "posts/challenge2_instructions.html#read-in-the-data",
    "href": "posts/challenge2_instructions.html#read-in-the-data",
    "title": "Challenge 2 Instructions",
    "section": "Read in the Data",
    "text": "Read in the Data\nRead in one (or more) of the following data sets, available in the posts/_data folder, using the correct R package and command.\n\nrailroad*.csv or StateCounty2012.xlsx ⭐\nFAOstat*.csv ⭐⭐⭐\nhotel_bookings ⭐⭐⭐⭐\n\n\n\n\nAdd any comments or documentation as needed. More challenging data may require additional code chunks and documentation."
  },
  {
    "objectID": "posts/challenge2_instructions.html#describe-the-data",
    "href": "posts/challenge2_instructions.html#describe-the-data",
    "title": "Challenge 2 Instructions",
    "section": "Describe the data",
    "text": "Describe the data\nUsing a combination of words and results of R commands, can you provide a high level description of the data? Describe as efficiently as possible where/how the data was (likely) gathered, indicate the cases and variables (both the interpretation and any details you deem useful to the reader to fully understand your chosen data)."
  },
  {
    "objectID": "posts/challenge2_instructions.html#provide-grouped-summary-statistics",
    "href": "posts/challenge2_instructions.html#provide-grouped-summary-statistics",
    "title": "Challenge 2 Instructions",
    "section": "Provide Grouped Summary Statistics",
    "text": "Provide Grouped Summary Statistics\nConduct some exploratory data analysis, using dplyr commands such as group_by(), select(), filter(), and summarise(). Find the central tendency (mean, median, mode) and dispersion (standard deviation, mix/max/quantile) for different subgroups within the data set.\n\n\n\n\nExplain and Interpret\nBe sure to explain why you choose a specific group. Comment on the interpretation of any interesting differences between groups that you uncover. This section can be integrated with the exploratory data analysis, just be sure it is included."
  },
  {
    "objectID": "posts/challenge1_MarcelaRobinson.html",
    "href": "posts/challenge1_MarcelaRobinson.html",
    "title": "Challenge 1",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)\n\n\n##Read the data Import data into R as a cvs file. This dataset contains information about the total number of railroad employees per county in the US.\n\n\nCode\nlibrary(readr)\nrailroad <- read.csv(\"C:/Users/mmrobinson/Desktop/Github/601_Winter_2022-2023/posts/_data/railroad_2012_clean_county.csv\")\n\n\nError in file(file, \"rt\"): cannot open the connection\n\n\nCode\nhead(railroad)\n\n\nError in head(railroad): object 'railroad' not found\n\n\n##Get dimension for railroad dataset This dataset has 3 different columns and 2930 rows.\n\n\nCode\ndim(railroad)\n\n\nError in eval(expr, envir, enclos): object 'railroad' not found\n\n\n##Get column names of railroad This dataset contains 3 columns named “state”, “county”, and “total_employees.”\n\n\nCode\ncolnames(railroad)\n\n\nError in is.data.frame(x): object 'railroad' not found\n\n\n##Select station_county and total_employees columns Focus on the total number of employees per county.\n\n\nCode\nselect(railroad, county, total_employees)\n\n\nError in select(railroad, county, total_employees): object 'railroad' not found\n\n\nCode\nhead(railroad)\n\n\nError in head(railroad): object 'railroad' not found\n\n\n##Arrange data by the total number of employees in descending order Arranging the dataset by the total number of employees in descending order help us determine that Cook, IL has the highest number of employees.\n\n\nCode\nrailroad %>%\nselect(total_employees, state, county) %>%\n  arrange(desc(total_employees))\n\n\nError in select(., total_employees, state, county): object 'railroad' not found\n\n\n##Arrange dataset based on the total number of employees per state\n\n\nCode\nrailroad%>%\n  arrange(desc(total_employees))%>%\n  select(total_employees,state, county) %>%\n  group_by(state) %>%\n    slice(1)%>%\n  arrange(desc(total_employees))\n\n\nError in arrange(., desc(total_employees)): object 'railroad' not found\n\n\n##Find the mean of total_employees per state with label\n\n\nCode\nsummarise(railroad, mean(total_employees))\n\n\nError in summarise(railroad, mean(total_employees)): object 'railroad' not found\n\n\n##Find basic statistical information on the dataset\n\n\nCode\nsummarise(railroad, mean.employees = mean(total_employees), min.employees = min(total_employees), max.employees = max(total_employees), sd.employees = sd(total_employees))\n\n\nError in summarise(railroad, mean.employees = mean(total_employees), min.employees = min(total_employees), : object 'railroad' not found\n\n\n##Find the mean of total_employees per state\n\n\nCode\nrailroad %>%\n  group_by(state) %>%\n  select(state, total_employees) %>%\n  summarise_all(mean, na.rm=TRUE)%>%\n  arrange(desc(total_employees))\n\n\nError in group_by(., state): object 'railroad' not found\n\n\n##Find the mean of total_employees per county Finding the mean and arranging the dataset by the total number of employees indicated that Delaware has the highest number of employees per station compared to the rest of the states. However, Texas has the most employees per state.\n\n\nCode\nrailroad%>%\n  group_by(state)%>%\n  select(total_employees)%>%\n  summarise_all(sum, na.rm = TRUE)%>%\narrange(desc(total_employees))\n\n\nError in group_by(., state): object 'railroad' not found\n\n\nConclusion - This dataset was likely created by the Railroad system in the United States to document the total number of railroad employees and keep record of its employment. The dataset contains 2930 observations and 3 variables. The county with most employees is in the state of Illinois, however, the state with most employees is in Texas."
  },
  {
    "objectID": "posts/challenge1_instructions.html",
    "href": "posts/challenge1_instructions.html",
    "title": "Challenge 1 Instructions",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge1_instructions.html#challenge-overview",
    "href": "posts/challenge1_instructions.html#challenge-overview",
    "title": "Challenge 1 Instructions",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to\n\nread in a dataset, and\ndescribe the dataset using both words and any supporting information (e.g., tables, etc)"
  },
  {
    "objectID": "posts/challenge1_instructions.html#read-in-the-data",
    "href": "posts/challenge1_instructions.html#read-in-the-data",
    "title": "Challenge 1 Instructions",
    "section": "Read in the Data",
    "text": "Read in the Data\nRead in one (or more) of the following data sets, using the correct R package and command.\n\nrailroad_2012_clean_county.csv ⭐\nbirds.csv ⭐⭐\nFAOstat*.csv ⭐⭐\nwild_bird_data.xlsx ⭐⭐⭐\nStateCounty2012.xlsx ⭐⭐⭐⭐\n\nFind the _data folder, located inside the posts folder. Then you can read in the data, using either one of the readr standard tidy read commands, or a specialized package such as readxl.\n\n\n\nAdd any comments or documentation as needed. More challenging data sets may require additional code chunks and documentation."
  },
  {
    "objectID": "posts/challenge1_instructions.html#describe-the-data",
    "href": "posts/challenge1_instructions.html#describe-the-data",
    "title": "Challenge 1 Instructions",
    "section": "Describe the data",
    "text": "Describe the data\nUsing a combination of words and results of R commands, can you provide a high level description of the data? Describe as efficiently as possible where/how the data was (likely) gathered, indicate the cases and variables (both the interpretation and any details you deem useful to the reader to fully understand your chosen data)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DACSS 601: Data Science Fundamentals - W22/23",
    "section": "",
    "text": "Challenge 1 Solution\n\n\n\n\n\n\n\nchallenge_1\n\n\nsolution\n\n\n\n\nReading in data and creating a post\n\n\n\n\n\n\nDec 26, 2022\n\n\nMeredith Rolfe & Sean Conway\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 1\n\n\n\n\n\n\n\nchallenge_1\n\n\n\n\nReading in data and creating a post\n\n\n\n\n\n\nDec 25, 2022\n\n\nMarcela Robinson\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 2 Instructions\n\n\n\n\n\n\n\nchallenge_2\n\n\n\n\nData wrangling: using group() and summarise()\n\n\n\n\n\n\nDec 21, 2022\n\n\nSean Conway\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChallenge 1 Instructions\n\n\n\n\n\n\n\nchallenge_1\n\n\n\n\nReading in data and creating a post\n\n\n\n\n\n\nDec 20, 2022\n\n\nSean Conway\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Import\n\n\n\n\n\n\n\ndata import\n\n\nreadr\n\n\n\n\nimporting data with R\n\n\n\n\n\n\nJun 5, 2022\n\n\nSean Conway\n\n\n\n\n\n\nNo matching items"
  }
]